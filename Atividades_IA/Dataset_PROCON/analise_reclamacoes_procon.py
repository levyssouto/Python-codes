# -*- coding: utf-8 -*-
"""Analise_Reclamacoes_Procon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XlWNItBfZf4ghTMyqEi64wrqDA5Vvpq5
"""

from google.colab import drive #conecta Google colab ao Drive
drive.mount('/content/drive')

#!pip install pandas_profiling

#!pip install sweetviz

import pandas as pd #importa bibliotecas necessárias à Análise de Dados
import numpy as np
import math
import matplotlib.pyplot as plt
import seaborn as sns

#import sweetviz as sv
#from pandas_profiling import ProfileReport

"""# **ANÁLISE EXPLORATÓRIA**"""

path = '/content/drive/MyDrive/Atividades_IA/base_arrumada.csv'

def inicia_dataset(path): #função para iniciar o dataset ou reiniciar caso necessário
  reclamacoes = pd.read_csv(path,sep=';',on_bad_lines='skip') #exclui badlines devido ao erro de tokenização
  return reclamacoes

reclamacoes=inicia_dataset(path) #inicia e reinicia dataset

print("Formato do dataset: ",reclamacoes.shape)
reclamacoes.head()

#Verificar tipos de dados de cada coluna e a quantidade de valores não nulos
print("Tipos de dados das colunas: \n")
reclamacoes.info()

print("Quantidade de valores faltantes no Dataset: \n") # verifica valores NULL
reclamacoes.isna().sum() #Eliminar arquivamento, abertura, faixa etaria, codigo assunto

"""### **ESTATÍSTICA DESCRITIVA E PREPARAÇÃO DE DADOS:**



"""

#Tirar colunas não necessárias á estatística descritiva
#excluir colunas com valores nominais
reclamacoes = inicia_dataset(path)
rec_estatistica = reclamacoes
rec_estatistica = rec_estatistica.sort_values('AnoCalendario')
remove_colunas = ["Tipo","CodigoProblema","strRazaoSocial","NumeroCNPJ","RadicalCNPJ","RazaoSocialRFB",
                  "NomeFantasiaRFB","CNAEPrincipal","DescCNAEPrincipal",
                  "CEPConsumidor","Unnamed: 23","Unnamed: 24"]
rec_estatistica.drop(remove_colunas,axis='columns',inplace=True)

#CONVERTER CAMPOS DE DATA DE INT PARA CHAR E DE CHAR PARA DATETIME
rec_estatistica['DataAbertura'] = pd.to_datetime(rec_estatistica['DataAbertura']) #converte pra datetime
rec_estatistica['DataAbertura'] = rec_estatistica['DataAbertura'].dt.strftime('%Y-%m-%d') #converte de volta pra string
rec_estatistica['DataAbertura'] = pd.to_datetime(rec_estatistica['DataAbertura']) #converte pra datetime


rec_estatistica['DataArquivamento'] = pd.to_datetime(rec_estatistica['DataArquivamento'])
rec_estatistica['DataArquivamento'] = rec_estatistica['DataArquivamento'].dt.strftime('%Y-%m-%d')
rec_estatistica['DataArquivamento'] = pd.to_datetime(rec_estatistica['DataArquivamento'])

rec_estatistica.info() # DataArquivamento e DataAbertura foram convertidos para datetime64

#Cria Análise do Dataset completo e deixa um arquivo .html na pasta para visualização
#profile = ProfileReport(rec_estatistica,title='Análise Exploratória')
#profile.to_file("rec_estatistica.html")

#report_sv = sv.analyze(rec_estatistica)
#report_sv.show_html("sw.html")

#empresa com mais reclamações: telefonia/vivo, problema mais recorrente: produto com vício, ano com mais reclamações: 2017
print("Tema com mais reclamações: ",rec_estatistica['DescricaoAssunto'].mode(),"\n")
print("Empresa com mais reclamações: ",rec_estatistica['strNomeFantasia'].mode(),"\n")
print("Problema mais recorrente: ",rec_estatistica['DescricaoProblema'].mode(),"\n")
print("Ano com mais reclamações: ",rec_estatistica['AnoCalendario'].mode(),"\n")

rec_estatistica['Atendida'] = rec_estatistica['Atendida'].map({'S' : 1, 'N': 0}) #gráfico de quantidade de pessoas atendidas e não atendidas, mapeando sim 1 0 não
rec_estatistica.hist(column='Atendida',color='green')

"""**CATEGORIZANDO ATRIBUTOS NOMINAIS**"""

#excluir colunas com valores nominais
remove_colunas_2 = ["strNomeFantasia","DescricaoAssunto","DescricaoProblema","Regiao"]
r_variavel=rec_estatistica
r_variavel.drop(remove_colunas_2,axis='columns',inplace=True) #remove colunas com atributos nominais

#Categorizar estados uf, de 1 e 27
estados = ["BA","CE","ES","GO","MA","MG","MT","MS","PA","PB","PE","PI","PR","RJ","RN","RO","RS","SC","SP","TO"]
j=0
for i in estados:
  j+=1
  r_variavel["UF"].replace(to_replace=i,value=j,inplace=True)


r_variavel.tail()

r_variavel['Atendida'].replace(to_replace='S',value=1,inplace=True) # SIM 1
r_variavel['Atendida'].replace(to_replace='N',value=0,inplace=True) # NÃO 0

r_variavel['SexoConsumidor'].replace(to_replace='F',value=1,inplace=True) # FEMININO 1
r_variavel['SexoConsumidor'].replace(to_replace='M',value=0,inplace=True) # MASCULINO 0

idades = ["até 20 anos","entre 21 a 30 anos","entre 31 a 40 anos","entre 41 a 50 anos","entre 51 a 60 anos","entre 61 a 70 anos","mais de 70 anos"]
j=0
for i in idades:
  j+=1
  r_variavel["FaixaEtariaConsumidor"].replace(to_replace=i,value=j,inplace=True)

'''Classificação Faixa Etária
    0 : até 20 anos
    1 : entre 21 a 30 anos
    2 : entre 31 a 40 anos
    3 : entre 41 a 50 anos
    4 : entre 51 a 60 anos
    5 : entre 61 a 70 anos
    6 : mais de 70 anos
'''

r_variavel['Dias'] = (r_variavel['DataArquivamento'] - r_variavel['DataAbertura']).dt.days #calcula dias e cria uma nova coluna
r_variavel['Meses'] = r_variavel['Dias']//30

'''CATEGORIZANDO CODIGO ASSUNTO:

#ALIMENTOS, BEBIDAS E DISTRIBUIDORAS = 1
#          FINANCAS,SEGURO E CRÉDITO = 2
#           MOBILARIA E CONSTRUTORA  = 3
#                    ARTIGOS DE CASA = 4
#             AUTOMOVEIS E AUTOPEÇAS = 5
#                 ARTIGOS PARA LAZER = 6
#                              SAUDE = 7
#                   ARTIGOS PESSOAIS = 8
#                           SERVIÇOS = 10
#                             OUTROS = 11
'''



categories = { # Cria categorias de assuntos por código
    'alimentos': list(range(1, 46)) + [50, 283],
    'financas': list(range(53, 76)) + [78, 79, 85, 86, 135, 144, 147, 229, 231, 280, 282],
    'mobiliaria': list(range(80, 85)) + [87],
    'domesticos': list(range(92, 99)) + list(range(100, 105)) + list(range(110, 113)) + list(range(120, 129)) + list(range(139, 144)) + [146],
    'auto': [99, 89, 129, 130, 131, 133, 145],
    'art_lazer': list(range(105, 110)) + list(range(245, 249)),
    'saude': [230, 284, 285, 286] + list(range(199, 225)),
    'art_pessoais': [132, 134, 287],
    'telecom': [136, 137, 186, 187, 188, 226, 227],
    'servicos': [281, 288] + list(range(156, 186)) + list(range(233, 245)) + list(range(249, 280)),
    'outros': [76, 77, 84, 88, 90, 91, 113, 114, 138]
}
i=0
for category, codes in categories.items():
    i+=1
    r_variavel['CodigoAssunto'].replace(to_replace=codes, value=i, inplace=True)


r_variavel.head()

"""TRATANDO DADOS FALTANTES"""

r_variavel.isna().sum() # conta a quantidade de valores
r_variavel.shape

r_variavel.replace("NULL",np.nan,inplace=True)
r_variavel.replace("Nao Informada",np.nan,inplace=True)
r_variavel.replace(["Desistência de compra (cancelamento de compra)","Produto com vício",
                    "Contrato/pedido/orçamento (rescisão, descumprimento, erro, etc.)","Produto entregue incompleto",
                    "SAC - Cancelamento de serviço (retenção, demora, não envio do comprovante)","Cobrança indevida/abusiva"],np.nan,inplace=True)
r_variavel.dropna(inplace=True)

#substitui valores nulos e errados no Dataset por NaN e exclui os registros

r_variavel['Dias'] = r_variavel['Dias'].astype(int) #substitui float por int na coluna de qtd de dias

r_variavel.drop(["DataArquivamento","DataAbertura"],axis='columns',inplace=True) #remove datas de colunas

print("formato atual dataset, sem valores nulos")
print(r_variavel.shape)
r_variavel.info()

r_variavel.head() # criar faixas de valores por dias a cada 180(semestre), classificar códigos do assunto

"""CRIANDO CLASSES DE ASSUNTO E DE TEMPO"""

dias = np.array(r_variavel['Dias'])

r_variavel['Semestres'] =  r_variavel['Dias'].apply(lambda x: math.ceil(x / 180) if (x % 180) > 90 else math.floor(x / 180))

r_variavel['Trimestres'] = r_variavel['Dias'].apply(lambda y: math.ceil(y / 90)if (y % 90) > 45 else math.floor(y / 90))

#cria colunas com número de semestres e trimestres baseados no dia
#usa list compreehension para percorrer valores de dias
#ao invés de varificar o resultado da divisão para arredondar, agor usamos operador % para verificar o resto, se maior que 90 arredonda pra cima, caso não
#arredonda pra baixo

r_variavel.drop("Dias",axis=1,inplace=True) #tira coluna dos dias
''' CÓDIGO REGIÃO
1: Norte
2: Nordeste
3: Sudeste
4: Sul
5: Centro-Oeste
'''

colunas_para_int = ['Atendida', 'CodigoAssunto', 'Meses', 'FaixaEtariaConsumidor', 'SexoConsumidor']

r_variavel[colunas_para_int] = r_variavel[colunas_para_int].astype(int)

r_variavel.head()

"""Correlações"""

r_variavel.corr().style.background_gradient(cmap='inferno') #mapa de correlação

fig,ax = plt.subplots(figsize=(8,5))
_=sns.heatmap(r_variavel.corr(),linewidth=0.4,annot=True)

"""# **TREINO E TESTE**"""

r_treino = r_variavel #inicia nova variavel para treino e teste

r_treino.describe().round(2)

anos = [2017,2018,2019,2020,2021]
j=0
for i in anos:
  j+=1
  r_treino["AnoCalendario"].replace(to_replace=i,value=j,inplace=True)
r_treino.head()

"""ÁRVORES DE DECISÃO"""

#features de treino = demais atributos
#rótulos =  Atendida

y = r_treino['Atendida']
r_treino.drop(['Atendida'],axis=1,inplace=True)
X = r_treino

y = np.array(y)
X = np.array(X)

r_treino.info()

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
#separa dados para treino e teste conforme porcentagem em test_size e define um estado aleatório para dataset
#X é o dataset com as features para treino e y a variável com rótulos 1 ou 0
X_train, X_test, y_train, y_test = train_test_split(X,
                                                   y, test_size=0.2,
                                                    random_state=42)

#modelo #arvore
arvore = DecisionTreeClassifier(max_depth=100,random_state=10) # variavel para modelo de arvore
modelo_arvore = arvore.fit(X_train, y_train) #variáveis de treino como parâmetro para modelo
predicao_arvore = modelo_arvore.predict(X_test) #guarda valores previstos em features de teste
predicao_arvore[0:10]

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(modelo_arvore, X_test, y_test, labels=[1,0])
plt.title("Matriz de Confusão: Árvores")
plt.show()

#mostra acurácia e taxa de acerto (score)
from sklearn.metrics import classification_report,accuracy_score
acuracia_atendimentos = accuracy_score(predicao_arvore, y_test) * 100

#relatório de classificação onde compara y_test (labels verdadeiras) e as predições realizadas, calcula métricas de avaliação
print(classification_report(y_test, predicao_arvore))

print("Taxa de Acerto é de %d%%" %acuracia_atendimentos)

"""XGB"""

from xgboost import XGBClassifier
modelo_xgb = XGBClassifier(n_estimators=100,learning_rate = 0.2, n_jobs = 4)
modelo_xgb.fit(X_train, y_train, early_stopping_rounds = 5, eval_set = [(X_test, y_test)], verbose = False)
y_pred = modelo_xgb.predict(X_test)


print(classification_report(y_test, y_pred))
acuracia_xgb = accuracy_score(y_test,y_pred)*100
print("Taxa de acerto de %d%%" %acuracia_xgb)

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(modelo_xgb, X_test, y_test, labels=[1,0])
plt.title("Matriz de Confusão: XGB")
plt.show()

"""REGRESSÃO LOGÍSTICA"""

#regressão logística
from sklearn.linear_model import LogisticRegression
regr_logistica = LogisticRegression() #treina modelo de regressao logistica
regr_logistica.fit(X_train, y_train)
y_pred_rl = regr_logistica.predict(X_test)

#taxas
print(classification_report(y_test, y_pred_rl))
score_rl = regr_logistica.score(X_test,y_test) *100
print("Taxa de Acerto é de %d%%" %score_rl)

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(regr_logistica, X_test, y_test, labels=[1,0])
plt.title("Matriz de Confusão: Regressão Logistica")
plt.show()

"""PERCEPTRON

"""

from sklearn.linear_model import Perceptron
# Criar e treinar o Perceptron
perceptron = Perceptron(random_state=101)
perceptron.fit(X_train, y_train)

# Fazer previsões no conjunto de teste
y_pred = perceptron.predict(X_test)

# Calcular a acurácia das previsões
acuracia_perceptron = accuracy_score(y_test, y_pred) * 100
print("Taxa de Acerto é de %d%%" %acuracia_perceptron)

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(perceptron, X_test, y_test, labels=[1,0])
plt.title("Matriz de Confusão: Perceptron")
plt.show()

"""FEED FORWARD"""

pip install tensorflow

import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#Normalizar dataset
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


feedf = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(9,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])


feedf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])


feedf.fit(X_train, y_train)

_,acuracia_feedf = feedf.evaluate(X_test, y_test)
print('Taxa de acerto: {:.2f}%'.format(acuracia_feedf * 100))